---
output: pdf_document
---

\begin{quote}
\emph{Any fool can know. The point is to understand.}
--- Albert Einstein
\end{quote}

\doublespacing

# Introduction

When the outcome variable is not continuous and/or has a distribution far from normal, researchers in prevention science generally use a generalized linear model (GLM). The power of GLMs is clear when you consider the broad range of situations it estimates with asymptotic consistency.\footnote{Asymptotic consistency refers to the ability to, as the sample size increases, produce estimates that converge to the proper value.} The problem with GLMs is that the estimates are not in an easily interpretable form. For example, in logistic regression (one type of GLM), the results are in "log-odds". To overcome this lack of interpretability, a simple exponentiation of the coefficient produces what is known as an odds ratio. Similarly, Poisson regression (another form of GLM), with an exponentiation, produces the risk ratio. Although some fields have adopted odds ratios (or relative risk, risk ratios, and other related metrics), these metrics have notable shortcomings.
\begin{enumerate}
\item Most of these metrics can be difficult to understand (i.e., many are not intuitive).
\item They cannot be combined with other metrics in a meaningful manner.
\end{enumerate}
\noindent The second is particularly important in the case of mediation analysis given the importance of the indirect effect (the combination of the $a$ and $b$ paths). If $a$ is not in a unit that can be combined with $b$, then obtaining a meaningful indirect effect is not generally possible.

## Problems with Odds Ratios and Others

Data have suggested that individuals, although with some variability, are able to intuitively grasp the meaning of phrases such as "highly probable" or "not likely" [@intelbook1999]. Yet, this same intuition is not found in odds or odds ratios. For example, @Montreuil2005 found, of 84 articles in several epidemiology journals that used odds ratios, only 7 (8.3\%) accurately interpreted the odds ratio and 22 (26\%) interpreted odds as though they were probabilities ("risk"). Figure \ref{fig:orprob} highlights the large discrepancy between the odds ratio, the risk ratio, and the average marginal risk. Ultimately, reporting odds and interpreting them as risk is common. Given its commonality and the limitation that it cannot be combined with other metrics, it is time to consider other strategies---at least in some situations.
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{OR_Prob.jpg}
  \caption{a) Comparison of odds ratio and risk ratio with the average marginal risk (probability). b) Same comparison as a) but the y-axis is rescaled ($log_{10}$) to better show the negative marginal risk comparisons. Both highlight the discrepancy between odds and risk ratios at various levels of marginal risk and that neither approximate the marginal risk.}
  \label{fig:orprob}
\end{figure}


# Additive vs. Multiplicative Interpretations

In most quantitative research designs, the investigators are seeking information on the average effect in a population, whether this refers to an average difference across groups or an average change in the outcome for a given change in the predictor. Generally speaking, the average effect is referring to the marginal effect (i.e., the effect of a small change in the predictor in the outcome's units). In the linear regression framework, the average effect is the estimated coefficient and is interpreted additively---a one unit change in the predictor is associated with an X unit change in the outcome. Conversely, outcomes such as OR are multiplicative. Being multiplicative changes the interpretation to: a one unit change in the predictor is associated with an X times change in the outcome. Although subtle, the difference is important, especially for multi-part models (e.g., mediation analysis). 

Being multiplicative indicates that the effect of the predictor changes based on the level of the predictor. For example, if the predictor is high, a small change in the predictor may have a big effect while if the predictor is low, a small change in the predictor has little effect. Figure \ref{fig:loglinear} shows this phenomenon, where, in the outcomes original units there is an exponential function. In part a) of the figure, it is clear that a change from 2 to 3 in the predictor has a much larger effect than a change from 0 to 1. A regression would not work well here. If a log transformation is used, the relationship would be linear (and a regression can be used) but the interpretation becomes multiplicative.

```{r loglinear, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(tidyverse)
set.seed(843)
d = data.frame("Predictor" = rnorm(100, 1)) %>%
  mutate(Outcome = exp(Predictor) + rnorm(100, sd=2))

p1 = ggplot(d, aes(x=Predictor, y=Outcome)) +
  geom_point(alpha = .9) +
  geom_smooth(se=FALSE, color = "firebrick3") +
  anteo::theme_anteo_wh() +
  labs(subtitle = "a)")
p2 = ggplot(d, aes(x=Predictor, y=Outcome)) +
  geom_point(alpha = .9) +
  geom_smooth(se=FALSE, color = "firebrick3") +
  anteo::theme_anteo_wh() +
  scale_y_continuous(trans = "log",
                     breaks = c(0, 1, 5, 10, 15, 20)) +
  labs(y = "Outcome (log transformation)",
       subtitle = "b)")
plot1 = gridExtra::grid.arrange(p1, p2, ncol = 2)
ggsave("loglinear.jpg", plot = plot1, height = 4, width=8, units = c("in"))
```
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{loglinear.jpg}
  \caption{Demonstration of a non-linear relationship. a) The outcome is an exponential function of the predictor. b) When log transforming the outcome, the relationship becomes fairly linear. So the interpretation in the log transformed scale is additive, but once it is put back into the original units, it is multiplicative.}
  \label{fig:loglinear}
\end{figure}

Additive interpretations are generally the most intuitive and require less cognitive resources to understand the pattern being portrayed. In a multiplicative framework, simplicity in understanding the effect intuitively is somewhat lost [@Iacobucci2008book]. Among others, this is one reason why Stata provides the `margins` command when dealing with two-part hurdle models.\footnote{These models are often used for zero-inflated count outcomes.} These models break up the modeling into two parts: one for the binary part and one for the count part. In order to combine the two parts, Stata allows a transformation known as the average marginal effect (discussed below) that makes the two parts both additive, and therefore easily combined (as is desired in mediation analysis). 

# Average Marginal Effect

## Why Consider Average Marginal Effects?

When using GLMs, the model is fit with a link function (e.g., "logit", "probit", "log"). This change causes the marginal effect of a variable to rely on the values of the covariates in the model. This is well illustrated through an example. Say a logistic regression model was fit to the data, as shown in Equations \ref{logisticmodel} - \ref{logisticmodel4}, for $p$ predictors. 
\begin{equation}\label{logisticmodel}
logit(Y_i) = \beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i
\end{equation}
\begin{equation}\label{logisticmodel2}
log(\frac{Prob(Y_i = 1)}{1 - Prob(Y_i = 1)}) = \beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i
\end{equation}
\begin{equation}\label{logisticmodel3}
\frac{Prob(Y_i = 1)}{1 - Prob(Y_i = 1)} = e^{\beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i}
\end{equation}
\begin{equation}\label{logisticmodel4}
Prob(Y_i = 1) = \frac{e^{\beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i}}{1 + e^{\beta_0 + \sum_{j=1}^p \beta_{j} X_i + \epsilon_i}}
\end{equation}
\noindent This implies that the marginal effect of, say, $X_{i1}$ is:
\begin{equation}\label{marginallogistic}
\frac{\delta Y}{\delta X_1} = \frac{e^{\beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i}}{(1 + e^{\beta_0 + \sum_{j=1}^p \beta_{j} X_{ij} + \epsilon_i})^2}
\end{equation}
\noindent That is, the marginal effect of the predictor $X_1$ *depends* on the level of each covariate (all $X_j$s) and the estimates (all $\beta_j$'s). This, understandably, complicates the interpretation. 

Importantly, this example makes it clear that each observational unit (e.g., individual) has a unique marginal effect given her observed levels of each variable. For example, if variable $X$ is increased by one from wherever it was observed, each individual will have different effects (i.e., different marginal effects). One individual may have a large effect; another small. That is, each individual has a marginal effect of a given predictor associated with her set of characteristics (covariates). As is done in many situations, one can find the mean of all such effects---thus obtaining the *average marginal effect* for that predictor.

The AME, then, is the *averaged* marginal effect across all observational units in the data. In this way, the results are representative of the effect seen in the sample. In linear models, the AME is the same as the original model estimates. This is intuitive given the AME is the marginal effect in the outcome's original units---the exact interpretation of the estimates in a linear model. However, as stated previously, in GLMs the estimates are not in the original units and therefore must be estimated via a post-estimation calculation described below.


## Definition of the Average Marginal Effect

In an instructive paper about the (now defunct) routine called "margeff" in Stata, @Bartus2005 highlights how the AME can be calculated---including the mathematical definition---and the benefits of AMEs compared to other related methods. The AME is a post-estimation calculation---it uses the model estimates and the data to provide the average effect. @Bartus2005 provided the definition of this post-estimation procedure of a continuous predictor as:
\begin{equation}
AME_{k} = \beta_k \frac{1}{n} \sum_{i=1}^n f(\beta X)
\end{equation}
\noindent where $f$ refers to the derivative of the estimate with respect to $x_k$, the $\beta X$ refers to the linear combination of the predictors (i.e., the predicted value), and $AME_k$ is the average marginal effect for the $kth$ variable. This definition provides the estimated change in the outcome for a one unit change in the continuous variable $x_k$ across all $n$ observations. 

Relatedly, the AME of a dummy coded variable is:
\begin{equation}
AME_{k} = \frac{1}{n} \sum_{i=1}^{n} \big[ F(\beta X | x_{ki} = 1) - F(\beta X | x_{ki} = 0) \big]
\end{equation}
\noindent where $F(\beta X | x_{ki} = 1)$ is the predicted value of the $ith$ observation when the dummy variable $x_k$ equals one and $F(\beta X | x_{ki} = 0)$ is the predicted value when the dummy value of $x_k$ equals zero holding all other variables constant. This, in effect, shows the discrete difference between the levels of the categorical variable in the outcome's original units.

## The Counter-Factual Framework

As mentioned previously, this project---including the use of average marginal effects---fits within the counter-factual framework. Indeed, the definition of a dummy coded variable demonstrates this well---$\big[ F(\beta X | x_i = 1) - F(\beta X | x_i = 0) \big]$. In essence, this asks: "What is the difference in the outcome when all observations of $x_k$ are equal to one vs. when all observations of $x_k$ are equal to zero?" The counter-factual framework asks the same question. When all assumptions are met, the AME is a direct, statistical answer to the causality conditions proposed by this framework.


## Confidence Intervals

In general, two approaches are taken to estimate the confidence intervals of AMEs. The first approach is the delta method, which provides standard errors [@Stata14]. Although beneficial, the second---bootstrapped confidence intervals---have proven accurate for understanding the variability in both the AME and mediation analysis. Therefore, this proposal uses bootstrapped confidence intervals.

# Interpretation

Table \ref{tab_int} presents the various units that the AME will produce for the various GLM links. It is important to note that all AMEs are in their original metrics whether they are probabilities, counts, or something else. The interpretation, then, is "for a one unit change in the predictor there is an associated [AME] change in the outcome."

```{r, results="asis"}
tab <- data.frame("Link Function"=c("Identity", "Log", "Logit", "Probit", "Poisson", "Gamma", "Negative Binomial"),
                  "Average Marginal Effect"=c("Original Unit", "Original Unit", "Risk", "Risk", "Count", "Count", "Count"))
names(tab) = c("Link Function", "Average Marginal Effect")
options(xtable.comment = FALSE)
library(xtable)
tabx = xtable(tab, 
       label = "tab_int", 
       caption = "The generalized linear model link functions with their associated units of interpretation. Note: This list is not exhaustive and there are likely more GLMs that are used within prevention research.",
       align = c("l", "|l", "|c|"))
print.xtable(tabx, include.rownames = FALSE, caption.placement = "top",
             table.placement = "tb")
```


## Benefits and Limitations

There are several benefits to using average marginal effects with GLMs.
\begin{itemize}
\item \emph{Intuitive Interpretation and Few Assumptions}. The first, and most obvious, benefit to using AMEs is the simplicity of the interpretation. The effect is in the units used in the modeling; it is additive (i.e., the effect is the added increase or subtracted decrease of the outcome). It provides an interpretation that imitates that of ordinary least squares regression. Relatedly, there are no difficult modeling assumptions directly tied to AMEs. Instead, the underlying models' assumptions that are used to get the AME is what is important. The only additional assumption with AME is that the effect, for each individual, is linear enough to be represented by an additive value and that the average adequately reports this.
\item \emph{More Generalizable and Robust}. There is evidence suggesting that AMEs are more robust to problems associated with GLMs (including logistic regression) such as unobserved heterogeneity and model mis-specification (Mood, 2010; Norton, 2012). This allows the estimates to be more generalizable to individuals outside of the sample.
\item \emph{Low Computational Burden}. Given AMEs are a post-estimation calculation, no new models need to be fit. Instead, using the estimates of the models, the average marginal effects can be calculated. The most burdensome of the calculation is the bootstrapped confidence intervals. 
\item \emph{Broadly Applicable}. The AME applies to any of the generalized linear models including logistic, Poisson, gamma, beta, negative binomial, and two-part hurdle models. This provides extensive flexibility in modeling, and, once applied to mediation, will allow flexibility in modeling based on the correct functional form.
\item \emph{Two-Part Models}. Particularly pertinent to this proposal is that the calculation of AMEs have been applied to two-part models, generally of the hurdle model types, as stated earlier. In fact, this is a common routine in the Stata statistical software. This provides valuable support for the proposed approach of using AMEs in mediation analysis.
\end{itemize}

Notably, the interpretation of AMEs hold to the assumption (as found in all regressions) that it is reasonable to adjust a single covariate while holding all others constant. This may not hold in reality, although it may be necessary to gain an understanding of the individual effect of a single variable. In data that are not representative of the population (e.g., non-random sample), AMEs may be biased because an over-representation of certain covariate values may be present. This is an overall modeling problem, since GLMs also assume a random sample. In this way, this problem is not specific to AMEs.

# Conclusions

The Average Marginal Effect produces intuitive, interpretable, and additive estimates of an effect. They have been applied to two-part models, not unlike mediation analysis, demonstrating their utility in difficult modeling situations. The following chapter discusses the integration of AME with mediation analysis---termed *Marginal Mediation Analysis*--- with its interpretation and assumptions, its benefits and limitations, and the basic procedures for its use.



\singlespacing
